{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autor: Efe Åžirin\n",
    "# Date: 2024-03-11\n",
    "\n",
    "# IMPORTS\n",
    "\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.], device='mps:0')\n",
      "MPS device found. - Apple Silicon GPU\n"
     ]
    }
   ],
   "source": [
    "# ======================= PYTHON SETTINGS ======================= #\n",
    "# =======================   GPU or CPU    ======================= #\n",
    "device = torch.device(\"cpu\")\n",
    "# Check if GPU is available -> CUDA\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"There are %d GPU(s) available.\" % torch.cuda.device_count())\n",
    "    print(\"We will use the GPU:\", torch.cuda.get_device_name(0))\n",
    "\n",
    "# Apple Silicon GPU\n",
    "if torch.backends.mps.is_available():\n",
    "    mps_device = torch.device(\"mps\")\n",
    "    out = torch.ones(1, device=mps_device)\n",
    "    print (out)\n",
    "    print (\"MPS device found. - Apple Silicon GPU\")\n",
    "else:\n",
    "    print (\"MPS device not found.\")\n",
    "\n",
    "# =======================   Ranom Seeds   ======================= #\n",
    "# Set random seed for reproducibility\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "# =============================================================== #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the training subset: 520\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define data directories\n",
    "data_dir = 'Dataset'\n",
    "train_dir = 'train'\n",
    "val_dir = 'val'\n",
    "test_dir = 'test'\n",
    "\n",
    "# Define transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),           # Convert images to PyTorch tensors\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # Normalize\n",
    "])\n",
    "\n",
    "# Load datasets\n",
    "train_dataset = torchvision.datasets.ImageFolder(root=os.path.join(data_dir, train_dir), transform=transform)\n",
    "fake_train_dataset = torchvision.datasets.ImageFolder(root= os.path.join(data_dir, train_dir), transform=transform)\n",
    "\n",
    "val_dataset = torchvision.datasets.ImageFolder(root=f'{data_dir}/{val_dir}', transform=transform)\n",
    "fake_val_dataset = torchvision.datasets.ImageFolder(root=f'{data_dir}/{val_dir}', transform=transform)\n",
    "\n",
    "test_dataset = torchvision.datasets.ImageFolder(root=f'{data_dir}/{test_dir}', transform=transform)\n",
    "fake_test_dataset = torchvision.datasets.ImageFolder(root=f'{data_dir}/{test_dir}', transform=transform)\n",
    "\n",
    "# Combine real and fake datasets for training, validation, and testing\n",
    "train_dataset = torch.utils.data.ConcatDataset([train_dataset, fake_train_dataset])\n",
    "val_dataset = torch.utils.data.ConcatDataset([val_dataset, fake_val_dataset])\n",
    "test_dataset = torch.utils.data.ConcatDataset([test_dataset, fake_test_dataset])\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# ! testing with subset of the data\n",
    "# Define the portion of data you want to use\n",
    "portion = 0.002  # for example, use 50% of the data\n",
    "\n",
    "# Determine the size of the subset\n",
    "subset_size_train = int(len(train_dataset) * portion)\n",
    "subset_size_val = int(len(val_dataset) * portion)\n",
    "subset_size_test = int(len(test_dataset) * portion)\n",
    "\n",
    "# Create subsets of the datasets\n",
    "train_dataset_subset, _ = torch.utils.data.random_split(train_dataset, [subset_size_train, len(train_dataset) - subset_size_train])\n",
    "val_dataset_subset, _ = torch.utils.data.random_split(val_dataset, [subset_size_val, len(val_dataset) - subset_size_val])\n",
    "test_dataset_subset, _ = torch.utils.data.random_split(test_dataset, [subset_size_test, len(test_dataset) - subset_size_test])\n",
    "\n",
    "print(f\"Size of the training subset: {len(train_dataset_subset)}\")\n",
    "\n",
    "# Create data loaders for the subsets\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset_subset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset_subset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset_subset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the CNN model\n",
    "class CNNClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNClassifier, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(64 * 64 * 64, 128)\n",
    "        self.fc2 = nn.Linear(128, 2)  # 2 classes: fake and real\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.conv1(x)))\n",
    "        x = self.pool(torch.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1)  # flatten all dimensions except batch\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 1.6545, Validation Loss: 0.6879, Validation Accuracy: 0.5625\n",
      "Epoch 2/10, Train Loss: 0.6935, Validation Loss: 0.6779, Validation Accuracy: 0.6250\n",
      "Epoch 3/10, Train Loss: 0.6688, Validation Loss: 0.6472, Validation Accuracy: 0.6875\n",
      "Epoch 4/10, Train Loss: 0.5896, Validation Loss: 0.6528, Validation Accuracy: 0.6250\n",
      "Epoch 5/10, Train Loss: 0.4807, Validation Loss: 0.7152, Validation Accuracy: 0.6875\n",
      "Epoch 6/10, Train Loss: 0.3206, Validation Loss: 0.8925, Validation Accuracy: 0.5312\n",
      "Epoch 7/10, Train Loss: 0.2002, Validation Loss: 0.7250, Validation Accuracy: 0.7500\n",
      "Epoch 8/10, Train Loss: 0.0926, Validation Loss: 0.9903, Validation Accuracy: 0.5000\n",
      "Epoch 9/10, Train Loss: 0.0620, Validation Loss: 1.1070, Validation Accuracy: 0.5938\n",
      "Epoch 10/10, Train Loss: 0.0186, Validation Loss: 1.1189, Validation Accuracy: 0.6250\n",
      "Test Accuracy: 0.7258\n",
      "Model saved to model.pth\n"
     ]
    }
   ],
   "source": [
    "# ======================== GPU TRAINING ======================== #\n",
    "# Initialize the model and move it to the GPU\n",
    "model = CNNClassifier().to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Define training function\n",
    "def train_model(model, criterion, optimizer, train_loader, val_loader, num_epochs=10):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)  # Move data to GPU\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)  # Move data to GPU\n",
    "                outputs = model(inputs)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item() * inputs.size(0)\n",
    "        val_loss /= len(val_loader.dataset)\n",
    "        accuracy = correct / total\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, \"\n",
    "              f\"Train Loss: {epoch_loss:.4f}, \"\n",
    "              f\"Validation Loss: {val_loss:.4f}, \"\n",
    "              f\"Validation Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Train the model\n",
    "train_model(model, criterion, optimizer, train_loader, val_loader, num_epochs=10)\n",
    "\n",
    "# Evaluate the model\n",
    "model.eval()\n",
    "test_correct = 0\n",
    "test_total = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)  # Move data to GPU\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        test_total += labels.size(0)\n",
    "        test_correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f\"Test Accuracy: {test_correct / test_total:.4f}\")\n",
    "\n",
    "# Save the model\n",
    "torch.save(model.state_dict(), 'model.pth')\n",
    "print(\"Model saved to model.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 2.3184, Validation Loss: 0.6923, Validation Accuracy: 0.5000\n",
      "Epoch 2/10, Train Loss: 0.6818, Validation Loss: 0.6617, Validation Accuracy: 0.5938\n",
      "Epoch 3/10, Train Loss: 0.6509, Validation Loss: 0.6155, Validation Accuracy: 0.6562\n",
      "Epoch 4/10, Train Loss: 0.5666, Validation Loss: 0.5534, Validation Accuracy: 0.7500\n",
      "Epoch 5/10, Train Loss: 0.4595, Validation Loss: 0.5049, Validation Accuracy: 0.8125\n",
      "Epoch 6/10, Train Loss: 0.3941, Validation Loss: 0.5946, Validation Accuracy: 0.7188\n",
      "Epoch 7/10, Train Loss: 0.2737, Validation Loss: 0.5352, Validation Accuracy: 0.7500\n",
      "Epoch 8/10, Train Loss: 0.1660, Validation Loss: 0.5458, Validation Accuracy: 0.8438\n",
      "Epoch 9/10, Train Loss: 0.0754, Validation Loss: 0.5901, Validation Accuracy: 0.8438\n",
      "Epoch 10/10, Train Loss: 0.0305, Validation Loss: 0.6871, Validation Accuracy: 0.8125\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize the model\n",
    "model = CNNClassifier()\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Define training function\n",
    "def train_model(model, criterion, optimizer, train_loader, val_loader, num_epochs=10):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                outputs = model(inputs)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item() * inputs.size(0)\n",
    "        val_loss /= len(val_loader.dataset)\n",
    "        accuracy = correct / total\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, \"\n",
    "              f\"Train Loss: {epoch_loss:.4f}, \"\n",
    "              f\"Validation Loss: {val_loss:.4f}, \"\n",
    "              f\"Validation Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Train the model\n",
    "train_model(model, criterion, optimizer, train_loader, val_loader, num_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.6935\n",
      "Model saved to model.pth\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Evaluate the model\n",
    "model.eval()\n",
    "test_correct = 0\n",
    "test_total = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        test_total += labels.size(0)\n",
    "        test_correct += (predicted == labels).sum().item()\n",
    "\n",
    "\n",
    "print(f\"Test Accuracy: {test_correct / test_total:.4f}\")\n",
    "\n",
    "# Save the model\n",
    "torch.save(model.state_dict(), 'model.pth')\n",
    "print(\"Model saved to model.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/efe/miniconda3/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/efe/miniconda3/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.7527, Validation Loss: 0.6898, Validation Accuracy: 0.5312\n",
      "Epoch 2/10, Train Loss: 0.6530, Validation Loss: 0.5950, Validation Accuracy: 0.7188\n",
      "Epoch 3/10, Train Loss: 0.5889, Validation Loss: 0.5655, Validation Accuracy: 0.7500\n",
      "Epoch 4/10, Train Loss: 0.5533, Validation Loss: 0.5397, Validation Accuracy: 0.7812\n",
      "Epoch 5/10, Train Loss: 0.5228, Validation Loss: 0.5062, Validation Accuracy: 0.7812\n",
      "Epoch 6/10, Train Loss: 0.4921, Validation Loss: 0.5002, Validation Accuracy: 0.8125\n",
      "Epoch 7/10, Train Loss: 0.5076, Validation Loss: 0.4805, Validation Accuracy: 0.8125\n",
      "Epoch 8/10, Train Loss: 0.5250, Validation Loss: 0.5071, Validation Accuracy: 0.7812\n",
      "Epoch 9/10, Train Loss: 0.4627, Validation Loss: 0.4376, Validation Accuracy: 0.8125\n",
      "Epoch 10/10, Train Loss: 0.4969, Validation Loss: 0.4404, Validation Accuracy: 0.8125\n"
     ]
    }
   ],
   "source": [
    "import torchvision.models as models\n",
    "\n",
    "# Define the CNN model\n",
    "class CNNClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNClassifier, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(64 * 64 * 64, 128)\n",
    "        self.fc2 = nn.Linear(128, 2)  # 2 classes: fake and real\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.conv1(x)))\n",
    "        x = self.pool(torch.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1)  # flatten all dimensions except batch\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Define training function with transfer learning\n",
    "def train_model_with_transfer_learning(backbone, criterion, optimizer, train_loader, val_loader, num_epochs=10):\n",
    "    for epoch in range(num_epochs):\n",
    "        backbone.train()\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = backbone(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        \n",
    "        # Validation\n",
    "        backbone.eval()\n",
    "        val_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                outputs = backbone(inputs)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item() * inputs.size(0)\n",
    "        val_loss /= len(val_loader.dataset)\n",
    "        accuracy = correct / total\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, \"\n",
    "              f\"Train Loss: {epoch_loss:.4f}, \"\n",
    "              f\"Validation Loss: {val_loss:.4f}, \"\n",
    "              f\"Validation Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Load a pre-trained ResNet-18 model\n",
    "resnet18 = models.resnet18(pretrained=True)\n",
    "\n",
    "# Freeze all the parameters in the pre-trained model\n",
    "for param in resnet18.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Modify the last fully connected layer to match the number of classes in your problem\n",
    "num_ftrs = resnet18.fc.in_features\n",
    "resnet18.fc = nn.Linear(num_ftrs, 2)  # 2 classes: fake and real\n",
    "\n",
    "# Create an instance of the transfer learning model with ResNet-18 backbone\n",
    "transfer_model = resnet18\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(transfer_model.parameters(), lr=0.001)\n",
    "\n",
    "# Define data loaders (Assuming train_loader and val_loader are already defined)\n",
    "\n",
    "# Train the model with transfer learning\n",
    "train_model_with_transfer_learning(transfer_model, criterion, optimizer, train_loader, val_loader, num_epochs=10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.8387\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Evaluate the model\n",
    "transfer_model.eval()\n",
    "test_correct = 0\n",
    "test_total = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = transfer_model(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        test_total += labels.size(0)\n",
    "        test_correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f\"Test Accuracy: {test_correct / test_total:.4f}\")\n",
    "\n",
    "# Save the model\n",
    "torch.save(transfer_model.state_dict(), 'transfer_model.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
